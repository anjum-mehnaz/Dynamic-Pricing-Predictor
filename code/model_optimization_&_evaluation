library(xgboost)

# Define all the predictor columns
predictor_columns <- c("Number_of_Riders", "Number_of_Drivers", "Location_Category",
                       "Customer_Loyalty_Status", "Number_of_Past_Rides",
                       "Average_Ratings", "Time_of_Booking", "Vehicle_Type",
                       "Expected_Ride_Duration",
                       "Ride_Density", "Surge_Indicator", "Peak_Hours_Flag")

# Create training data matrix
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, predictor_columns]), label = train_data$Historical_Cost_of_Ride)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, predictor_columns]), label = test_data$Historical_Cost_of_Ride)

# Set up the caret training grid
xgb_grid <- expand.grid(
  nrounds = c(50, 100),         # number of boosting rounds
  max_depth = c(3, 6, 9),       # depth of trees
  eta = c(0.1, 0.3),            # learning rate
  gamma = 0,                    # minimum loss reduction
  colsample_bytree = 1,         # subsample ratio of columns
  min_child_weight = 1,         # minimum sum of instance weight
  subsample = 1                 # subsample ratio of training instance
)

# Train control
xgb_control <- trainControl(method = "cv", number = 5, verboseIter = FALSE)

# Train the model
set.seed(123)
xgb_tuned <- train(
  x = train_data[, predictor_columns],
  y = train_data$Historical_Cost_of_Ride,
  method = "xgbTree",
  trControl = xgb_control,
  tuneGrid = xgb_grid,
  verbose = FALSE
)

# Evaluate on test set
xgb_best_model <- xgb_tuned$finalModel
xgb_preds <- predict(xgb_tuned, newdata = test_data[, predictor_columns])
xgb_mae <- mean(abs(xgb_preds - test_data$Historical_Cost_of_Ride))
xgb_r2 <- cor(xgb_preds, test_data$Historical_Cost_of_Ride)^2

cat("Tuned XGBoost MAE:", xgb_mae, "R²:", xgb_r2, "\n")

library(randomForest)
library(caret)

# Define predictor columns
predictor_columns <- c("Number_of_Riders", "Number_of_Drivers", "Location_Category",
                       "Customer_Loyalty_Status", "Number_of_Past_Rides", "Average_Ratings",
                       "Time_of_Booking", "Vehicle_Type", "Expected_Ride_Duration",
                       "Ride_Density", "Surge_Indicator", "Peak_Hours_Flag")

# Set up tuning grid for Random Forest
rf_grid <- expand.grid(mtry = c(2, 4, 6, 8))  # number of variables randomly sampled at each split

# Set up train control for cross-validation
rf_control <- trainControl(method = "cv", number = 5, verboseIter = FALSE)

# Train the Random Forest model
set.seed(123)
rf_tuned <- train(
  x = train_data[, predictor_columns],
  y = train_data$Historical_Cost_of_Ride,
  method = "rf",
  trControl = rf_control,
  tuneGrid = rf_grid,
  ntree = 100,
  importance = TRUE
)

# Evaluate on test set
rf_preds <- predict(rf_tuned, newdata = test_data[, predictor_columns])
rf_mae <- mean(abs(rf_preds - test_data$Historical_Cost_of_Ride))
rf_r2 <- cor(rf_preds, test_data$Historical_Cost_of_Ride)^2

cat("Tuned Random Forest MAE:", rf_mae, "R²:", rf_r2, "\n")

# Model comparison summary
cat("Model Comparison Summary:\n")
cat("-------------------------------------------------\n")
cat("Previous Models:\n")
cat("Random Forest - MAE:", 55.93038, "R²:", 0.8754674, "\n")
cat("XGBoost (Before Tuning) - MAE:", 57.23552, "R²:", 0.8515301, "\n")
cat("-------------------------------------------------\n")
cat("Tuned Models:\n")
cat("Tuned Random Forest - MAE:", 51.7977, "R²:", 0.8793644, "\n")
cat("Tuned XGBoost - MAE:", 51.78446, "R²:", 0.884614, "\n")
cat("-------------------------------------------------\n")
cat("Conclusion: The tuned XGBoost model performs the best with the lowest MAE and highest R²,\n")
cat("closely followed by the tuned Random Forest. Tuning significantly improved both models' performance.\n")
